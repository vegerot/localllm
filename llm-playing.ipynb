{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a9207bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787fbcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between (King - Man + Woman) and Queen: 0.7330\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = ollama.embeddings(model=\"embeddinggemma:300m\", prompt=text)\n",
    "    return np.array(response[\"embedding\"])\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "\n",
    "# 1. Generate embeddings\n",
    "king = get_embedding(\"king\")\n",
    "man = get_embedding(\"man\")\n",
    "woman = get_embedding(\"woman\")\n",
    "queen = get_embedding(\"queen\")\n",
    "\n",
    "# 2. Perform the arithmetic: King - Man + Woman\n",
    "# We are essentially \"subtracting\" the concept of masculinity\n",
    "# and \"adding\" the concept of femininity to the concept of royalty.\n",
    "masculinity_vector = man - woman\n",
    "result_vector = king - masculinity_vector\n",
    "\n",
    "# 3. Verify the result\n",
    "similarity = cosine_similarity(king, queen)\n",
    "\n",
    "print(f\"Similarity between (King - Man + Woman) and Queen: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c53c88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Ollama model: gemma3:270M\n",
      "Using manifest: /home/max/.ollama/models/manifests/registry.ollama.ai/library/gemma3/270M\n",
      "Using GGUF: /home/max/.ollama/models/blobs/sha256-735af2139dc652bf01112746474883d79a52fa1c19038265d363e3d42556f7a2\n",
      "1) Load the GGUF and access the full token embedding matrix (quantized)\n",
      "Vocab size: 262144\n",
      "Embedding dim: 640\n",
      "Embedding quant type: 8\n",
      "2) Load tokenizer tokens so we can decode token IDs back to text\n",
      "No valid chat template found in GGUF; trying Ollama CLI...\n",
      "context['chat_template']='{{- $systemPromptAdded := false }}\\n{{- range $i, $_ := .Messages }}\\n{{- $last := eq (len (slice $.Messages $i)) 1 }}\\n{{- if eq .Role \"user\" }}<start_of_turn>user\\n{{- if (and (not $systemPromptAdded) $.System) }}\\n{{- $systemPromptAdded = true }}\\n{{ $.System }}\\n{{ end }}\\n{{ .Content }}<end_of_turn>\\n{{ if $last }}<start_of_turn>model\\n{{ end }}\\n{{- else if eq .Role \"assistant\" }}<start_of_turn>model\\n{{ .Content }}{{ if not $last }}<end_of_turn>\\n{{ end }}\\n{{- end }}\\n{{- end }}'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import heapq\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional, Tuple, Union, cast\n",
    "\n",
    "import numpy as np\n",
    "import gguf\n",
    "\n",
    "# Use an Ollama model reference, e.g. \"qwen3:0.6b\" or \"gemma3:27B\".\n",
    "# This notebook reads the local Ollama GGUF blob directly (no HuggingFace downloads).\n",
    "\n",
    "OLLAMA_MODEL = \"gemma3:270M\"\n",
    "\n",
    "MANIFESTS_ROOT = Path.home() / \".ollama/models/manifests\"\n",
    "BLOBS_ROOT = Path.home() / \".ollama/models/blobs\"\n",
    "\n",
    "\n",
    "def _parse_ollama_model_ref(model: str) -> Tuple[str, str, str, str]:\n",
    "    \"\"\"Parse Ollama model reference into (host, namespace, repo_path, tag).\"\"\"\n",
    "    model = model.strip()\n",
    "    if not model:\n",
    "        raise ValueError(\"Empty OLLAMA_MODEL\")\n",
    "\n",
    "    if \":\" in model:\n",
    "        repo_part, tag = model.rsplit(\":\", 1)\n",
    "        tag = tag or \"latest\"\n",
    "    else:\n",
    "        repo_part, tag = model, \"latest\"\n",
    "\n",
    "    parts = [p for p in repo_part.split(\"/\") if p]\n",
    "    if len(parts) >= 3:\n",
    "        host = parts[0]\n",
    "        namespace = parts[1]\n",
    "        repo_path = \"/\".join(parts[2:])\n",
    "    elif len(parts) == 2:\n",
    "        host = \"registry.ollama.ai\"\n",
    "        namespace = parts[0]\n",
    "        repo_path = parts[1]\n",
    "    else:\n",
    "        host = \"registry.ollama.ai\"\n",
    "        namespace = \"library\"\n",
    "        repo_path = parts[0]\n",
    "\n",
    "    return host, namespace, repo_path, tag\n",
    "\n",
    "\n",
    "def _candidate_manifest_paths(model: str) -> List[Path]:\n",
    "    host, namespace, repo_path, tag = _parse_ollama_model_ref(model)\n",
    "    repo_parts = repo_path.split(\"/\")\n",
    "\n",
    "    candidates: List[Path] = []\n",
    "    direct = MANIFESTS_ROOT / host / namespace\n",
    "    for rp in repo_parts:\n",
    "        direct = direct / rp\n",
    "    direct = direct / tag\n",
    "    candidates.append(direct)\n",
    "\n",
    "    # Fallback: match any manifest that ends with /<repo_last>/<tag>.\n",
    "    repo_last = repo_parts[-1]\n",
    "    candidates.extend(MANIFESTS_ROOT.glob(f\"**/{repo_last}/{tag}\"))\n",
    "\n",
    "    # De-duplicate preserving order\n",
    "    seen: set[Path] = set()\n",
    "    out: List[Path] = []\n",
    "    for p in candidates:\n",
    "        if p not in seen:\n",
    "            seen.add(p)\n",
    "            out.append(p)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _resolve_ollama_manifest(model: str) -> Path:\n",
    "    candidates = _candidate_manifest_paths(model)\n",
    "    existing = [p for p in candidates if p.is_file()]\n",
    "    if not existing:\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not find an Ollama manifest for model \"\n",
    "            f\"{model!r} under {MANIFESTS_ROOT}. Tried:\\n\"\n",
    "            + \"\\n\".join(str(p) for p in candidates[:10])\n",
    "        )\n",
    "    if len(existing) == 1:\n",
    "        return existing[0]\n",
    "    existing.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return existing[0]\n",
    "\n",
    "\n",
    "def _resolve_ollama_gguf_blob(manifest_path: Path) -> Path:\n",
    "    obj = json.loads(manifest_path.read_text(encoding=\"utf-8\"))\n",
    "    for layer in obj.get(\"layers\", []):\n",
    "        if layer.get(\"mediaType\") == \"application/vnd.ollama.image.model\":\n",
    "            digest = layer[\"digest\"]  # e.g. sha256:...\n",
    "            if not digest.startswith(\"sha256:\"):\n",
    "                raise ValueError(f\"Unexpected digest format: {digest}\")\n",
    "            blob = BLOBS_ROOT / (\"sha256-\" + digest.split(\":\", 1)[1])\n",
    "            if not blob.exists():\n",
    "                raise FileNotFoundError(f\"Resolved blob does not exist: {blob}\")\n",
    "            return blob\n",
    "    raise ValueError(f\"No model layer found in manifest: {manifest_path}\")\n",
    "\n",
    "\n",
    "def _assert_gguf_file(path: Path) -> None:\n",
    "    with path.open(\"rb\") as f:\n",
    "        magic = f.read(4)\n",
    "    assert magic == b\"GGUF\", f\"Expected GGUF file, got magic={magic!r} at {path}\"\n",
    "\n",
    "\n",
    "def _tok_to_str(x: Union[str, bytes]) -> str:\n",
    "    if isinstance(x, bytes):\n",
    "        return x.decode(\"utf-8\", errors=\"replace\")\n",
    "    return str(x)\n",
    "\n",
    "\n",
    "def _field_to_str(field) -> Optional[str]:\n",
    "    if not field:\n",
    "        return None\n",
    "    data = getattr(field, \"data\", None)\n",
    "    if isinstance(data, list) and data:\n",
    "        value = data[0]\n",
    "        if isinstance(value, bytes):\n",
    "            return value.decode(\"utf-8\", errors=\"replace\")\n",
    "        return str(value)\n",
    "    if isinstance(data, bytes):\n",
    "        return data.decode(\"utf-8\", errors=\"replace\")\n",
    "    if isinstance(data, (str, int, float)):\n",
    "        return str(data)\n",
    "    try:\n",
    "        contents = field.contents()\n",
    "    except Exception:\n",
    "        return None\n",
    "    if isinstance(contents, list) and contents:\n",
    "        value = contents[0]\n",
    "        if isinstance(value, bytes):\n",
    "            return value.decode(\"utf-8\", errors=\"replace\")\n",
    "        return str(value)\n",
    "    if isinstance(contents, bytes):\n",
    "        return contents.decode(\"utf-8\", errors=\"replace\")\n",
    "    if isinstance(contents, (str, int, float)):\n",
    "        return str(contents)\n",
    "    return None\n",
    "\n",
    "\n",
    "def _parse_chat_template_value(value: str) -> Optional[str]:\n",
    "    s = value.strip()\n",
    "    if not s or s.isdigit():\n",
    "        return None\n",
    "    if s[0] in \"[{\":\n",
    "        try:\n",
    "            obj = json.loads(s)\n",
    "        except Exception:\n",
    "            obj = None\n",
    "        if isinstance(obj, dict):\n",
    "            tmpl = obj.get(\"template\")\n",
    "            if tmpl is not None:\n",
    "                return str(tmpl)\n",
    "            return None\n",
    "        if isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                if (\n",
    "                    isinstance(item, dict)\n",
    "                    and item.get(\"name\") == \"default\"\n",
    "                    and \"template\" in item\n",
    "                ):\n",
    "                    return str(item[\"template\"])\n",
    "            for item in obj:\n",
    "                if isinstance(item, dict) and \"template\" in item:\n",
    "                    return str(item[\"template\"])\n",
    "            return None\n",
    "    return s\n",
    "\n",
    "\n",
    "def _looks_like_template(text: str) -> bool:\n",
    "    t = text.strip()\n",
    "    if not t:\n",
    "        return False\n",
    "    if t.isdigit():\n",
    "        return False\n",
    "    if \"{{\" in t and \"Messages\" in t:\n",
    "        return True\n",
    "    if \"<start_of_turn>\" in t or \"<|im_start|>\" in t:\n",
    "        return True\n",
    "    if \"add_generation_prompt\" in t:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _get_chat_template_from_gguf(reader: gguf.GGUFReader) -> Optional[str]:\n",
    "    field = reader.fields.get(\"tokenizer.chat_template\")\n",
    "    raw = _field_to_str(field)\n",
    "    if raw:\n",
    "        template = _parse_chat_template_value(raw)\n",
    "        if template and _looks_like_template(template):\n",
    "            return template\n",
    "\n",
    "    for key, field in reader.fields.items():\n",
    "        if \"chat_template\" not in key:\n",
    "            continue\n",
    "        raw = _field_to_str(field)\n",
    "        if not raw:\n",
    "            continue\n",
    "        template = _parse_chat_template_value(raw)\n",
    "        if template and _looks_like_template(template):\n",
    "            return template\n",
    "\n",
    "    for field in reader.fields.values():\n",
    "        raw = _field_to_str(field)\n",
    "        if not raw:\n",
    "            continue\n",
    "        template = _parse_chat_template_value(raw)\n",
    "        if template and _looks_like_template(template):\n",
    "            return template\n",
    "    return None\n",
    "\n",
    "\n",
    "def _get_chat_template_name_from_gguf(reader: gguf.GGUFReader) -> Optional[str]:\n",
    "    field = reader.fields.get(\"tokenizer.chat_template\")\n",
    "    raw = _field_to_str(field)\n",
    "    if not raw:\n",
    "        return None\n",
    "    name = raw.strip()\n",
    "    if not name or name.isdigit():\n",
    "        return None\n",
    "    if name[0] in \"[{\":\n",
    "        return None\n",
    "    if _looks_like_template(name):\n",
    "        return None\n",
    "    return name\n",
    "\n",
    "\n",
    "def _get_ollama_repo_template(\n",
    "    template_name: str, repo_root: Optional[Path] = None\n",
    ") -> Optional[str]:\n",
    "    if not template_name:\n",
    "        return None\n",
    "    repo_root = repo_root or (Path.home() / \"workspace/github.com/ollama/ollama\")\n",
    "    template_dir = repo_root / \"template\"\n",
    "    if not template_dir.is_dir():\n",
    "        return None\n",
    "    if template_name.endswith(\".gotmpl\"):\n",
    "        candidates = [template_name]\n",
    "    else:\n",
    "        candidates = [f\"{template_name}.gotmpl\", f\"{template_name}-chat.gotmpl\"]\n",
    "    for name in candidates:\n",
    "        path = template_dir / name\n",
    "        if path.is_file():\n",
    "            return path.read_text(encoding=\"utf-8\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def _get_ollama_chat_template(model: str) -> Optional[str]:\n",
    "    import ollama\n",
    "\n",
    "    info = ollama.show(model)\n",
    "    template = str(info.get(\"template\"))\n",
    "    return str(template)\n",
    "\n",
    "\n",
    "def load_ollama_gguf_context(model: str) -> dict:\n",
    "    manifest_path = _resolve_ollama_manifest(model)\n",
    "    gguf_path = _resolve_ollama_gguf_blob(manifest_path)\n",
    "    _assert_gguf_file(gguf_path)\n",
    "\n",
    "    print(\"Using Ollama model:\", model)\n",
    "    print(\"Using manifest:\", manifest_path)\n",
    "    print(\"Using GGUF:\", gguf_path)\n",
    "\n",
    "    # ---- Embedding-matrix utilities (GGUF) ----\n",
    "    print(\"1) Load the GGUF and access the full token embedding matrix (quantized)\")\n",
    "    reader = gguf.GGUFReader(str(gguf_path))\n",
    "\n",
    "    # token_embd.weight = embedding table (token id -> vector)\n",
    "    token_tensor = next(t for t in reader.tensors if t.name == \"token_embd.weight\")\n",
    "    vocab_size = token_tensor.data.shape[0]\n",
    "\n",
    "    # Determine embedding dimensionality from a dequantized row (robust across architectures).\n",
    "    embed_dim = int(\n",
    "        gguf.dequantize(token_tensor.data[:1], token_tensor.tensor_type).shape[1]\n",
    "    )\n",
    "\n",
    "    print(f\"Vocab size: {vocab_size}\")\n",
    "    print(f\"Embedding dim: {embed_dim}\")\n",
    "    print(f\"Embedding quant type: {token_tensor.tensor_type}\")\n",
    "\n",
    "    print(\"2) Load tokenizer tokens so we can decode token IDs back to text\")\n",
    "    tok_field = reader.fields[\"tokenizer.ggml.tokens\"]\n",
    "    tokens_raw: Union[List[str], List[bytes]] = tok_field.contents()\n",
    "\n",
    "    tokens: List[str] = [_tok_to_str(x) for x in tokens_raw]\n",
    "\n",
    "    chat_template = _get_chat_template_from_gguf(reader)\n",
    "    if not chat_template:\n",
    "        template_name = _get_chat_template_name_from_gguf(reader)\n",
    "        if template_name:\n",
    "            chat_template = _get_ollama_repo_template(template_name)\n",
    "    if not chat_template:\n",
    "        print(\"No valid chat template found in GGUF; trying Ollama CLI...\")\n",
    "        chat_template = _get_ollama_chat_template(model)\n",
    "\n",
    "    return {\n",
    "        \"reader\": reader,\n",
    "        \"gguf_path\": gguf_path,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"embed_dim\": embed_dim,\n",
    "        \"tokens\": tokens,\n",
    "        \"token_tensor\": token_tensor,\n",
    "        \"chat_template\": chat_template,\n",
    "    }\n",
    "\n",
    "\n",
    "# %load_ext line_profiler\n",
    "# %lprun -f load_ollama_gguf_context context = load_ollama_gguf_context(OLLAMA_MODEL)\n",
    "context = load_ollama_gguf_context(OLLAMA_MODEL)\n",
    "print(f\"{context['chat_template']=}\")\n",
    "\n",
    "\n",
    "def find_token_id(word: str, tokens: List[str]) -> int:\n",
    "    \"\"\"Best-effort lookup for a *single token* ID (not full tokenization).\"\"\"\n",
    "    candidates = [word, \"Ġ\" + word, \"▁\" + word, \" \" + word]\n",
    "    for c in candidates:\n",
    "        try:\n",
    "            return tokens.index(c)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    raise ValueError(\n",
    "        f\"Could not find a single-token match for {word!r}. Try another word.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def dequantize_rows(start: int, end: int, token_tensor) -> np.ndarray:\n",
    "    \"\"\"Dequantize embedding rows [start:end) to float32 shape [N, embed_dim].\"\"\"\n",
    "    q = token_tensor.data[start:end]\n",
    "    return gguf.dequantize(q, token_tensor.tensor_type)\n",
    "\n",
    "\n",
    "def get_token_vector(token_id: int, token_tensor) -> np.ndarray:\n",
    "    return dequantize_rows(token_id, token_id + 1, token_tensor)[0]\n",
    "\n",
    "\n",
    "def get_token_vector_str(token: str, tokens: List[str], token_tensor) -> np.ndarray:\n",
    "    tid = find_token_id(token, tokens)\n",
    "    return get_token_vector(tid, token_tensor)\n",
    "\n",
    "\n",
    "def normalize(v: np.ndarray) -> np.ndarray:\n",
    "    n = np.linalg.norm(v)\n",
    "    return v if n == 0 else (v / n)\n",
    "\n",
    "\n",
    "def find_closest_tokens(\n",
    "    vector: np.ndarray,\n",
    "    vocab_size: int,\n",
    "    token_tensor,\n",
    "    tokens: List[str],\n",
    "    k: int = 5,\n",
    "    ignore_ids: Optional[Iterable[int]] = None,\n",
    "    chunk_size: int = 2048,\n",
    ") -> List[Tuple[str, float, int]]:\n",
    "    \"\"\"\n",
    "    Find top-k cosine-similar tokens across the *entire* vocab by dequantizing in chunks.\n",
    "    Returns (token_string, similarity, token_id).\n",
    "    \"\"\"\n",
    "    ignore = set(ignore_ids or [])\n",
    "    target = normalize(vector).astype(np.float32, copy=False)\n",
    "\n",
    "    heap: List[Tuple[float, int]] = []  # (score, token_id), min-heap\n",
    "\n",
    "    for start in range(0, vocab_size, chunk_size):\n",
    "        end = min(start + chunk_size, vocab_size)\n",
    "        emb = dequantize_rows(start, end, token_tensor)  # float32  [N, embed_dim]\n",
    "        norms = np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "        norms[norms == 0] = 1.0\n",
    "        emb = emb / norms\n",
    "        scores = emb @ target  # float32 [N]\n",
    "\n",
    "        for i, score in enumerate(scores):\n",
    "            tid = start + i\n",
    "            if tid in ignore:\n",
    "                continue\n",
    "            s = float(score)\n",
    "            if len(heap) < k:\n",
    "                heapq.heappush(heap, (s, tid))\n",
    "            elif s > heap[0][0]:\n",
    "                heapq.heapreplace(heap, (s, tid))\n",
    "\n",
    "    best = sorted(heap, reverse=True)\n",
    "    return [(tokens[tid], score, tid) for score, tid in best]\n",
    "\n",
    "\n",
    "# ---- Chat-like function (local GGUF) ----\n",
    "# Note: `gguf` provides reading/dequantization utilities but does not execute the model forward pass.\n",
    "# For inference on a GGUF model we use llama.cpp via `llama-cpp-python`, still pointing at Ollama's local GGUF blob.\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant.\"\n",
    "\n",
    "_LLAMA = None\n",
    "\n",
    "\n",
    "def _render_prompt(messages: List[dict[str, str]]) -> str:\n",
    "    \"\"\"Render a prompt using GGUF's chat template when available; fallback to a simple ChatML-ish format.\"\"\"\n",
    "    if context[\"chat_template\"]:\n",
    "        try:\n",
    "            from jinja2 import Environment, StrictUndefined\n",
    "\n",
    "            def raise_exception(msg: str):\n",
    "                raise RuntimeError(msg)\n",
    "\n",
    "            env = Environment(\n",
    "                undefined=StrictUndefined, trim_blocks=True, lstrip_blocks=True\n",
    "            )\n",
    "            j = env.from_string(context[\"chat_template\"])\n",
    "            return j.render(\n",
    "                messages=messages,\n",
    "                add_generation_prompt=True,\n",
    "                raise_exception=raise_exception,\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    parts: List[str] = []\n",
    "    for m in messages:\n",
    "        parts.append(f\"<|im_start|>{m['role']}\\n{m['content']}\\n<|im_end|>\")\n",
    "    parts.append(\"<|im_start|>assistant\\n\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "def _get_llama(gguf_path: Path):\n",
    "    global _LLAMA\n",
    "    if _LLAMA is not None:\n",
    "        return _LLAMA\n",
    "    try:\n",
    "        from llama_cpp import Llama\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"Missing dependency: llama-cpp-python. Install it in this notebook kernel, e.g.\\n\"\n",
    "            \"  pip install llama-cpp-python\\n\\n\"\n",
    "            \"This still uses the Ollama-downloaded GGUF file locally and does not use the ollama Python package.\"\n",
    "        ) from e\n",
    "\n",
    "    n_ctx = 4096\n",
    "    n_threads = max(1, (os.cpu_count() or 4) - 1)\n",
    "    _LLAMA = Llama(\n",
    "        model_path=str(gguf_path),\n",
    "        n_ctx=n_ctx,\n",
    "        n_threads=n_threads,\n",
    "        logits_all=False,\n",
    "        vocab_only=False,\n",
    "    )\n",
    "    return _LLAMA\n",
    "\n",
    "\n",
    "def next_message(user_message: str, messages: List[dict[str, str]]) -> str:\n",
    "    \"\"\"\n",
    "    Chat-like helper similar to `ollama.chat`, but using the local Ollama-downloaded GGUF blob.\n",
    "    Does not import or call the `ollama` Python package.\n",
    "    \"\"\"\n",
    "    msg = user_message.strip()\n",
    "    if not msg:\n",
    "        return \"\"\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": msg})\n",
    "    prompt = _render_prompt(messages)\n",
    "\n",
    "    llm = _get_llama(context[\"gguf_path\"])\n",
    "    out = llm.create_completion(\n",
    "        prompt=prompt,\n",
    "        max_tokens=256,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        stop=[\"<|im_end|>\", \"</s>\"],\n",
    "    )\n",
    "    out = cast(dict, out)\n",
    "    text = (out.get(\"choices\", [{}])[0].get(\"text\") or \"\").strip()\n",
    "    messages.append({\"role\": \"assistant\", \"content\": text})\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "722311c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Give me a 1-sentence summary of what you can do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 60 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =      94.50 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     307.13 ms /    20 runs   (   15.36 ms per token,    65.12 tokens per second)\n",
      "llama_perf_context_print:       total time =     315.25 ms /    21 tokens\n",
      "llama_perf_context_print:    graphs reused =         18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Okay, I understand. Please tell me what you can do.\n"
     ]
    }
   ],
   "source": [
    "# Example usage of `next_message` (chat-like helper backed by the local GGUF)\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "\n",
    "prompt = \"Give me a 1-sentence summary of what you can do.\"\n",
    "print(\"User:\", prompt)\n",
    "print(\"Assistant:\", next_message(prompt, messages))\n",
    "# print()\n",
    "\n",
    "# print(\n",
    "#     \"User: Write a Python function that computes cosine similarity between two numpy vectors.\"\n",
    "# )\n",
    "# print(\n",
    "#     \"Assistant:\",\n",
    "#     next_message(\n",
    "#         \"Write a Python function that computes cosine similarity between two numpy vectors.\",\n",
    "#         messages\n",
    "#     ),\n",
    "# )\n",
    "# print()\n",
    "\n",
    "# print(\"User: Now explain in 2 bullet points how the prompt is constructed here.\")\n",
    "# print(\n",
    "#     \"Assistant:\",\n",
    "#     next_message(\n",
    "#         \"Now explain in 2 bullet points how the prompt is constructed here.\",\n",
    "#         messages\n",
    "#     ),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12635dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3) Demo\n",
      "3a) find token IDs for inputs\n",
      "3b) find closest tokens\n",
      "\n",
      "Closest matches in the entire vocabulary:\n",
      "1. '<unused18>' (id=24, similarity=0.0000)\n",
      "2. '<unused17>' (id=23, similarity=0.0000)\n",
      "3. '<unused16>' (id=22, similarity=0.0000)\n",
      "4. '<unused15>' (id=21, similarity=0.0000)\n",
      "5. '<unused14>' (id=20, similarity=0.0000)\n",
      "6. '<unused13>' (id=19, similarity=0.0000)\n",
      "7. '<unused12>' (id=18, similarity=0.0000)\n",
      "8. '<unused11>' (id=17, similarity=0.0000)\n",
      "9. '<unused10>' (id=16, similarity=0.0000)\n",
      "10. '<unused9>' (id=15, similarity=0.0000)\n",
      "11. '<unused8>' (id=14, similarity=0.0000)\n",
      "12. '<unused7>' (id=13, similarity=0.0000)\n",
      "13. '<unused6>' (id=12, similarity=0.0000)\n",
      "14. '<unused5>' (id=11, similarity=0.0000)\n",
      "15. '<unused4>' (id=10, similarity=0.0000)\n",
      "16. '<unused3>' (id=9, similarity=0.0000)\n",
      "17. '<unused2>' (id=8, similarity=0.0000)\n",
      "18. '<unused1>' (id=7, similarity=0.0000)\n",
      "19. '<unused0>' (id=6, similarity=0.0000)\n",
      "20. '[multimodal]' (id=5, similarity=0.0000)\n",
      "21. '<mask>' (id=4, similarity=0.0000)\n",
      "22. '<unk>' (id=3, similarity=0.0000)\n",
      "23. '<bos>' (id=2, similarity=0.0000)\n",
      "24. '<eos>' (id=1, similarity=0.0000)\n",
      "25. '<pad>' (id=0, similarity=0.0000)\n",
      "\n",
      "Longest token in vocabulary is 31 characters: '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      "\n",
      "Top 10 longest tokens in vocabulary:\n",
      "1. 31 chars: '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      "2. 31 chars: '                               '\n",
      "3. 31 chars: '\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
      "4. 30 chars: '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      "5. 30 chars: '                              '\n",
      "6. 30 chars: '\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
      "7. 29 chars: '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      "8. 29 chars: '                             '\n",
      "9. 29 chars: '\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
      "10. 28 chars: '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "# 3) Demo: king - man + woman (single-token lookup)\n",
    "print(\"3) Demo\")\n",
    "print(\"3a) find token IDs for inputs\")\n",
    "king_id = find_token_id(\"king\", context[\"tokens\"])\n",
    "man_id = find_token_id(\"man\", context[\"tokens\"])\n",
    "woman_id = find_token_id(\"woman\", context[\"tokens\"])\n",
    "\n",
    "king_vec = get_token_vector(king_id, context[\"token_tensor\"])\n",
    "queen_vec = get_token_vector(\n",
    "    find_token_id(\"queen\", context[\"tokens\"]), context[\"token_tensor\"]\n",
    ")\n",
    "boy_vec = get_token_vector(\n",
    "    find_token_id(\"boy\", context[\"tokens\"]), context[\"token_tensor\"]\n",
    ")\n",
    "\n",
    "man_vec = get_token_vector(man_id, context[\"token_tensor\"])\n",
    "woman_vec = get_token_vector(woman_id, context[\"token_tensor\"])\n",
    "masculinity_vector = man_vec - woman_vec\n",
    "\n",
    "target_vector = queen_vec + masculinity_vector\n",
    "target_vector = get_token_vector_str(\n",
    "    \"emoji\", context[\"tokens\"], context[\"token_tensor\"]\n",
    ") - get_token_vector_str(\n",
    "    \"emoji\", context[\"tokens\"], context[\"token_tensor\"]\n",
    ")  # + get_token_vector_str(\"apple\", context['tokens'], context['token_tensor']) #+ get_token_vector_str(\"Playstation\", context['tokens'], context['token_tensor'])\n",
    "\n",
    "ignore = []\n",
    "print(\"3b) find closest tokens\")\n",
    "matches = find_closest_tokens(\n",
    "    target_vector,\n",
    "    k=25,\n",
    "    ignore_ids=ignore,\n",
    "    vocab_size=context[\"vocab_size\"],\n",
    "    token_tensor=context[\"token_tensor\"],\n",
    "    tokens=context[\"tokens\"],\n",
    ")\n",
    "\n",
    "print(\"\\nClosest matches in the entire vocabulary:\")\n",
    "for i, (tok, score, tid) in enumerate(matches, start=1):\n",
    "    print(f\"{i}. {tok!r} (id={tid}, similarity={score:.4f})\")\n",
    "\n",
    "longest_token = max(context[\"tokens\"], key=lambda t: len(t))\n",
    "print(\n",
    "    f\"\\nLongest token in vocabulary is {len(longest_token)} characters: {longest_token!r}\"\n",
    ")\n",
    "\n",
    "ten_longest_tokens = heapq.nlargest(10, context[\"tokens\"], key=lambda t: len(t))\n",
    "print(\"\\nTop 10 longest tokens in vocabulary:\")\n",
    "for i, tok in enumerate(ten_longest_tokens, start=1):\n",
    "    print(f\"{i}. {len(tok)} chars: {tok!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646902b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
