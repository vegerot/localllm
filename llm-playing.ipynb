{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a9207bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787fbcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between (King - Man + Woman) and Queen: 0.7330\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = ollama.embeddings(model=\"embeddinggemma:300m\", prompt=text)\n",
    "    return np.array(response[\"embedding\"])\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "\n",
    "# 1. Generate embeddings\n",
    "king = get_embedding(\"king\")\n",
    "man = get_embedding(\"man\")\n",
    "woman = get_embedding(\"woman\")\n",
    "queen = get_embedding(\"queen\")\n",
    "\n",
    "# 2. Perform the arithmetic: King - Man + Woman\n",
    "# We are essentially \"subtracting\" the concept of masculinity\n",
    "# and \"adding\" the concept of femininity to the concept of royalty.\n",
    "masculinity_vector = man - woman\n",
    "result_vector = king - masculinity_vector\n",
    "\n",
    "# 3. Verify the result\n",
    "similarity = cosine_similarity(king, queen)\n",
    "\n",
    "print(f\"Similarity between (King - Man + Woman) and Queen: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c53c88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Ollama model: translategemma:4b\n",
      "Using manifest: /home/max/.ollama/models/manifests/registry.ollama.ai/library/translategemma/4b\n",
      "Using GGUF: /home/max/.ollama/models/blobs/sha256-bdbf939b402e2f88fbe3e918beb777813009335756b4c17be7fe008dfe4815d4\n",
      "1) Load the GGUF and access the full token embedding matrix (quantized)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function LlamaModel.__del__ at 0x7ffa7c9672e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/max/workspace/github.com/vegerot/localllm/.venv/lib/python3.11/site-packages/llama_cpp/_internals.py\", line 86, in __del__\n",
      "    self.close()\n",
      "  File \"/home/max/workspace/github.com/vegerot/localllm/.venv/lib/python3.11/site-packages/llama_cpp/_internals.py\", line 78, in close\n",
      "    if self.sampler is not None:\n",
      "       ^^^^^^^^^^^^\n",
      "AttributeError: 'LlamaModel' object has no attribute 'sampler'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 262208\n",
      "Embedding dim: 2560\n",
      "Embedding quant type: 14\n",
      "2) Load tokenizer tokens so we can decode token IDs back to text\n",
      "No chat template found in GGUF; checking for named template to read directly from Ollama source code...\n",
      "No valid chat template found in GGUF; trying Ollama library...\n",
      "context['chat_template']='{{- range $i, $_ := .Messages }}\\n{{- $last := eq (len (slice $.Messages $i)) 1 }}\\n{{- if or (eq .Role \"user\") (eq .Role \"system\") }}<start_of_turn>user\\n{{ .Content }}<end_of_turn>\\n{{ if $last }}<start_of_turn>model\\n{{ end }}\\n{{- else if eq .Role \"assistant\" }}<start_of_turn>model\\n{{ .Content }}{{ if not $last }}<end_of_turn>\\n{{ end }}\\n{{- end }}\\n{{- end }}'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import heapq\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional, Tuple, Union, cast\n",
    "\n",
    "import numpy as np\n",
    "import gguf\n",
    "\n",
    "# Use an Ollama model reference, e.g. \"qwen3:0.6b\" or \"gemma3:27B\".\n",
    "# This notebook reads the local Ollama GGUF blob directly (no HuggingFace downloads).\n",
    "\n",
    "OLLAMA_MODEL = \"translategemma:4b\"\n",
    "\n",
    "MANIFESTS_ROOT = Path.home() / \".ollama/models/manifests\"\n",
    "BLOBS_ROOT = Path.home() / \".ollama/models/blobs\"\n",
    "\n",
    "\n",
    "def _parse_ollama_model_ref(model: str) -> Tuple[str, str, str, str]:\n",
    "    \"\"\"Parse Ollama model reference into (host, namespace, repo_path, tag).\"\"\"\n",
    "    model = model.strip()\n",
    "    if not model:\n",
    "        raise ValueError(\"Empty OLLAMA_MODEL\")\n",
    "\n",
    "    if \":\" in model:\n",
    "        repo_part, tag = model.rsplit(\":\", 1)\n",
    "        tag = tag or \"latest\"\n",
    "    else:\n",
    "        repo_part, tag = model, \"latest\"\n",
    "\n",
    "    parts = [p for p in repo_part.split(\"/\") if p]\n",
    "    if len(parts) >= 3:\n",
    "        host = parts[0]\n",
    "        namespace = parts[1]\n",
    "        repo_path = \"/\".join(parts[2:])\n",
    "    elif len(parts) == 2:\n",
    "        host = \"registry.ollama.ai\"\n",
    "        namespace = parts[0]\n",
    "        repo_path = parts[1]\n",
    "    else:\n",
    "        host = \"registry.ollama.ai\"\n",
    "        namespace = \"library\"\n",
    "        repo_path = parts[0]\n",
    "\n",
    "    return host, namespace, repo_path, tag\n",
    "\n",
    "\n",
    "def _candidate_manifest_paths(model: str) -> List[Path]:\n",
    "    host, namespace, repo_path, tag = _parse_ollama_model_ref(model)\n",
    "    repo_parts = repo_path.split(\"/\")\n",
    "\n",
    "    candidates: List[Path] = []\n",
    "    direct = MANIFESTS_ROOT / host / namespace\n",
    "    for rp in repo_parts:\n",
    "        direct = direct / rp\n",
    "    direct = direct / tag\n",
    "    candidates.append(direct)\n",
    "\n",
    "    # Fallback: match any manifest that ends with /<repo_last>/<tag>.\n",
    "    repo_last = repo_parts[-1]\n",
    "    candidates.extend(MANIFESTS_ROOT.glob(f\"**/{repo_last}/{tag}\"))\n",
    "\n",
    "    # De-duplicate preserving order\n",
    "    seen: set[Path] = set()\n",
    "    out: List[Path] = []\n",
    "    for p in candidates:\n",
    "        if p not in seen:\n",
    "            seen.add(p)\n",
    "            out.append(p)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _resolve_ollama_manifest(model: str) -> Path:\n",
    "    candidates = _candidate_manifest_paths(model)\n",
    "    existing = [p for p in candidates if p.is_file()]\n",
    "    if not existing:\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not find an Ollama manifest for model \"\n",
    "            f\"{model!r} under {MANIFESTS_ROOT}. Tried:\\n\"\n",
    "            + \"\\n\".join(str(p) for p in candidates[:10])\n",
    "        )\n",
    "    if len(existing) == 1:\n",
    "        return existing[0]\n",
    "    existing.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return existing[0]\n",
    "\n",
    "\n",
    "def _resolve_ollama_gguf_blob(manifest_path: Path) -> Path:\n",
    "    obj = json.loads(manifest_path.read_text(encoding=\"utf-8\"))\n",
    "    for layer in obj.get(\"layers\", []):\n",
    "        if layer.get(\"mediaType\") == \"application/vnd.ollama.image.model\":\n",
    "            digest = layer[\"digest\"]  # e.g. sha256:...\n",
    "            if not digest.startswith(\"sha256:\"):\n",
    "                raise ValueError(f\"Unexpected digest format: {digest}\")\n",
    "            blob = BLOBS_ROOT / (\"sha256-\" + digest.split(\":\", 1)[1])\n",
    "            if not blob.exists():\n",
    "                raise FileNotFoundError(f\"Resolved blob does not exist: {blob}\")\n",
    "            return blob\n",
    "    raise ValueError(f\"No model layer found in manifest: {manifest_path}\")\n",
    "\n",
    "\n",
    "def _assert_gguf_file(path: Path) -> None:\n",
    "    with path.open(\"rb\") as f:\n",
    "        magic = f.read(4)\n",
    "    assert magic == b\"GGUF\", f\"Expected GGUF file, got magic={magic!r} at {path}\"\n",
    "\n",
    "\n",
    "def _tok_to_str(x: Union[str, bytes]) -> str:\n",
    "    if isinstance(x, bytes):\n",
    "        return x.decode(\"utf-8\", errors=\"replace\")\n",
    "    return str(x)\n",
    "\n",
    "\n",
    "def _field_to_str(field) -> Optional[str]:\n",
    "    if not field:\n",
    "        return None\n",
    "    data = getattr(field, \"data\", None)\n",
    "    if isinstance(data, list) and data:\n",
    "        value = data[0]\n",
    "        if isinstance(value, bytes):\n",
    "            return value.decode(\"utf-8\", errors=\"replace\")\n",
    "        return str(value)\n",
    "    if isinstance(data, bytes):\n",
    "        return data.decode(\"utf-8\", errors=\"replace\")\n",
    "    if isinstance(data, (str, int, float)):\n",
    "        return str(data)\n",
    "    try:\n",
    "        contents = field.contents()\n",
    "    except Exception:\n",
    "        return None\n",
    "    if isinstance(contents, list) and contents:\n",
    "        value = contents[0]\n",
    "        if isinstance(value, bytes):\n",
    "            return value.decode(\"utf-8\", errors=\"replace\")\n",
    "        return str(value)\n",
    "    if isinstance(contents, bytes):\n",
    "        return contents.decode(\"utf-8\", errors=\"replace\")\n",
    "    if isinstance(contents, (str, int, float)):\n",
    "        return str(contents)\n",
    "    return None\n",
    "\n",
    "\n",
    "def _parse_chat_template_value(value: str) -> Optional[str]:\n",
    "    s = value.strip()\n",
    "    if not s or s.isdigit():\n",
    "        return None\n",
    "    if s[0] in \"[{\":\n",
    "        try:\n",
    "            obj = json.loads(s)\n",
    "        except Exception:\n",
    "            obj = None\n",
    "        if isinstance(obj, dict):\n",
    "            tmpl = obj.get(\"template\")\n",
    "            if tmpl is not None:\n",
    "                return str(tmpl)\n",
    "            return None\n",
    "        if isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                if (\n",
    "                    isinstance(item, dict)\n",
    "                    and item.get(\"name\") == \"default\"\n",
    "                    and \"template\" in item\n",
    "                ):\n",
    "                    return str(item[\"template\"])\n",
    "            for item in obj:\n",
    "                if isinstance(item, dict) and \"template\" in item:\n",
    "                    return str(item[\"template\"])\n",
    "            return None\n",
    "    return s\n",
    "\n",
    "\n",
    "def _looks_like_template(text: str) -> bool:\n",
    "    t = text.strip()\n",
    "    if not t:\n",
    "        return False\n",
    "    if t.isdigit():\n",
    "        return False\n",
    "    if \"{{\" in t and \"Messages\" in t:\n",
    "        return True\n",
    "    if \"<start_of_turn>\" in t or \"<|im_start|>\" in t:\n",
    "        return True\n",
    "    if \"add_generation_prompt\" in t:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _get_chat_template_from_gguf(reader: gguf.GGUFReader) -> Optional[str]:\n",
    "    field = reader.fields.get(\"tokenizer.chat_template\")\n",
    "    raw = _field_to_str(field)\n",
    "    if raw:\n",
    "        template = _parse_chat_template_value(raw)\n",
    "        if template and _looks_like_template(template):\n",
    "            return template\n",
    "\n",
    "    for key, field in reader.fields.items():\n",
    "        if \"chat_template\" not in key:\n",
    "            continue\n",
    "        raw = _field_to_str(field)\n",
    "        if not raw:\n",
    "            continue\n",
    "        template = _parse_chat_template_value(raw)\n",
    "        if template and _looks_like_template(template):\n",
    "            return template\n",
    "\n",
    "    for field in reader.fields.values():\n",
    "        raw = _field_to_str(field)\n",
    "        if not raw:\n",
    "            continue\n",
    "        template = _parse_chat_template_value(raw)\n",
    "        if template and _looks_like_template(template):\n",
    "            return template\n",
    "    return None\n",
    "\n",
    "\n",
    "def _get_chat_template_name_from_gguf(reader: gguf.GGUFReader) -> Optional[str]:\n",
    "    field = reader.fields.get(\"tokenizer.chat_template\")\n",
    "    raw = _field_to_str(field)\n",
    "    if not raw:\n",
    "        return None\n",
    "    name = raw.strip()\n",
    "    if not name or name.isdigit():\n",
    "        return None\n",
    "    if name[0] in \"[{\":\n",
    "        return None\n",
    "    if _looks_like_template(name):\n",
    "        return None\n",
    "    return name\n",
    "\n",
    "\n",
    "def _get_ollama_repo_template(\n",
    "    template_name: str, repo_root: Optional[Path] = None\n",
    ") -> Optional[str]:\n",
    "    if not template_name:\n",
    "        return None\n",
    "    repo_root = repo_root or (Path.home() / \"workspace/github.com/ollama/ollama\")\n",
    "    template_dir = repo_root / \"template\"\n",
    "    if not template_dir.is_dir():\n",
    "        return None\n",
    "    if template_name.endswith(\".gotmpl\"):\n",
    "        candidates = [template_name]\n",
    "    else:\n",
    "        candidates = [f\"{template_name}.gotmpl\", f\"{template_name}-chat.gotmpl\"]\n",
    "    for name in candidates:\n",
    "        path = template_dir / name\n",
    "        if path.is_file():\n",
    "            return path.read_text(encoding=\"utf-8\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def _get_ollama_chat_template(model: str) -> Optional[str]:\n",
    "    import ollama\n",
    "\n",
    "    info = ollama.show(model)\n",
    "    template = str(info.get(\"template\"))\n",
    "    return str(template)\n",
    "\n",
    "\n",
    "def load_ollama_gguf_context(model: str) -> dict:\n",
    "    manifest_path = _resolve_ollama_manifest(model)\n",
    "    gguf_path = _resolve_ollama_gguf_blob(manifest_path)\n",
    "    _assert_gguf_file(gguf_path)\n",
    "\n",
    "    print(\"Using Ollama model:\", model)\n",
    "    print(\"Using manifest:\", manifest_path)\n",
    "    print(\"Using GGUF:\", gguf_path)\n",
    "\n",
    "    # ---- Embedding-matrix utilities (GGUF) ----\n",
    "    print(\"1) Load the GGUF and access the full token embedding matrix (quantized)\")\n",
    "    reader = gguf.GGUFReader(str(gguf_path))\n",
    "\n",
    "    # token_embd.weight = embedding table (token id -> vector)\n",
    "    token_tensor = next(t for t in reader.tensors if t.name == \"token_embd.weight\")\n",
    "    vocab_size = token_tensor.data.shape[0]\n",
    "\n",
    "    # Determine embedding dimensionality from a dequantized row (robust across architectures).\n",
    "    embed_dim = int(\n",
    "        gguf.dequantize(token_tensor.data[:1], token_tensor.tensor_type).shape[1]\n",
    "    )\n",
    "\n",
    "    print(f\"Vocab size: {vocab_size}\")\n",
    "    print(f\"Embedding dim: {embed_dim}\")\n",
    "    print(f\"Embedding quant type: {token_tensor.tensor_type}\")\n",
    "\n",
    "    print(\"2) Load tokenizer tokens so we can decode token IDs back to text\")\n",
    "    tok_field = reader.fields[\"tokenizer.ggml.tokens\"]\n",
    "    tokens_raw: Union[List[str], List[bytes]] = tok_field.contents()\n",
    "\n",
    "    tokens: List[str] = [_tok_to_str(x) for x in tokens_raw]\n",
    "\n",
    "    chat_template = _get_chat_template_from_gguf(reader)\n",
    "    if not chat_template:\n",
    "        print(\n",
    "            \"No chat template found in GGUF; checking for named template to read directly from Ollama source code...\"\n",
    "        )\n",
    "        template_name = _get_chat_template_name_from_gguf(reader)\n",
    "        if template_name:\n",
    "            chat_template = _get_ollama_repo_template(template_name)\n",
    "    if not chat_template:\n",
    "        print(\"No valid chat template found in GGUF; trying Ollama library...\")\n",
    "        chat_template = _get_ollama_chat_template(model)\n",
    "\n",
    "    return {\n",
    "        \"reader\": reader,\n",
    "        \"gguf_path\": gguf_path,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"embed_dim\": embed_dim,\n",
    "        \"tokens\": tokens,\n",
    "        \"token_tensor\": token_tensor,\n",
    "        \"chat_template\": chat_template,\n",
    "    }\n",
    "\n",
    "\n",
    "# %load_ext line_profiler\n",
    "# %lprun -f load_ollama_gguf_context context = load_ollama_gguf_context(OLLAMA_MODEL)\n",
    "context = load_ollama_gguf_context(OLLAMA_MODEL)\n",
    "print(f\"{context['chat_template']=}\")\n",
    "\n",
    "\n",
    "def find_token_id(word: str, tokens: List[str]) -> int:\n",
    "    \"\"\"Best-effort lookup for a *single token* ID (not full tokenization).\"\"\"\n",
    "    candidates = [word, \"Ġ\" + word, \"▁\" + word, \" \" + word]\n",
    "    for c in candidates:\n",
    "        try:\n",
    "            return tokens.index(c)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    raise ValueError(\n",
    "        f\"Could not find a single-token match for {word!r}. Try another word.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def dequantize_rows(start: int, end: int, token_tensor) -> np.ndarray:\n",
    "    \"\"\"Dequantize embedding rows [start:end) to float32 shape [N, embed_dim].\"\"\"\n",
    "    q = token_tensor.data[start:end]\n",
    "    return gguf.dequantize(q, token_tensor.tensor_type)\n",
    "\n",
    "\n",
    "def get_token_vector(token_id: int, token_tensor) -> np.ndarray:\n",
    "    return dequantize_rows(token_id, token_id + 1, token_tensor)[0]\n",
    "\n",
    "\n",
    "def get_token_vector_str(token: str) -> np.ndarray:\n",
    "    tid = find_token_id(token, context[\"tokens\"])\n",
    "    return get_token_vector(tid, context[\"token_tensor\"])\n",
    "\n",
    "\n",
    "def normalize(v: np.ndarray) -> np.ndarray:\n",
    "    n = np.linalg.norm(v)\n",
    "    return v if n == 0 else (v / n)\n",
    "\n",
    "\n",
    "def find_closest_tokens(\n",
    "    vector: np.ndarray,\n",
    "    vocab_size: int,\n",
    "    token_tensor,\n",
    "    tokens: List[str],\n",
    "    k: int = 5,\n",
    "    ignore_ids: Optional[Iterable[int]] = None,\n",
    "    chunk_size: int = 2048,\n",
    ") -> List[Tuple[str, float, int]]:\n",
    "    \"\"\"\n",
    "    Find top-k cosine-similar tokens across the *entire* vocab by dequantizing in chunks.\n",
    "    Returns (token_string, similarity, token_id).\n",
    "    \"\"\"\n",
    "    ignore = set(ignore_ids or [])\n",
    "    target = normalize(vector).astype(np.float32, copy=False)\n",
    "\n",
    "    heap: List[Tuple[float, int]] = []  # (score, token_id), min-heap\n",
    "\n",
    "    for start in range(0, vocab_size, chunk_size):\n",
    "        end = min(start + chunk_size, vocab_size)\n",
    "        emb = dequantize_rows(start, end, token_tensor)  # float32  [N, embed_dim]\n",
    "        norms = np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "        norms[norms == 0] = 1.0\n",
    "        emb = emb / norms\n",
    "        scores = emb @ target  # float32 [N]\n",
    "\n",
    "        for i, score in enumerate(scores):\n",
    "            tid = start + i\n",
    "            if tid in ignore:\n",
    "                continue\n",
    "            s = float(score)\n",
    "            if len(heap) < k:\n",
    "                heapq.heappush(heap, (s, tid))\n",
    "            elif s > heap[0][0]:\n",
    "                heapq.heapreplace(heap, (s, tid))\n",
    "\n",
    "    best = sorted(heap, reverse=True)\n",
    "    return [(tokens[tid], score, tid) for score, tid in best]\n",
    "\n",
    "\n",
    "# ---- Chat-like function (local GGUF) ----\n",
    "# Note: `gguf` provides reading/dequantization utilities but does not execute the model forward pass.\n",
    "# For inference on a GGUF model we use llama.cpp via `llama-cpp-python`, still pointing at Ollama's local GGUF blob.\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant.\"\n",
    "\n",
    "_LLAMA = None\n",
    "\n",
    "\n",
    "def _extract_system_message(messages: List[dict[str, str]]) -> str:\n",
    "    for m in messages:\n",
    "        if m.get(\"role\") == \"system\":\n",
    "            return m.get(\"content\", \"\")\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def _is_gemma3_template(template: str) -> bool:\n",
    "    return (\n",
    "        \"<start_of_turn>\" in template\n",
    "        and \"$systemPromptAdded\" in template\n",
    "        and \".Messages\" in template\n",
    "    )\n",
    "\n",
    "\n",
    "def _render_gemma3_template(messages: List[dict[str, str]]) -> str:\n",
    "    system_text = _extract_system_message(messages)\n",
    "    usable = [m for m in messages if m.get(\"role\") in (\"user\", \"assistant\")]\n",
    "    parts: List[str] = []\n",
    "    system_added = False\n",
    "    for i, m in enumerate(usable):\n",
    "        last = i == len(usable) - 1\n",
    "        role = m.get(\"role\")\n",
    "        content = m.get(\"content\", \"\")\n",
    "        if role == \"user\":\n",
    "            parts.append(\"<start_of_turn>user\")\n",
    "            if (not system_added) and system_text:\n",
    "                system_added = True\n",
    "                parts.append(system_text)\n",
    "            parts.append(content)\n",
    "            parts.append(\"<end_of_turn>\")\n",
    "            if last:\n",
    "                parts.append(\"<start_of_turn>model\")\n",
    "        elif role == \"assistant\":\n",
    "            parts.append(\"<start_of_turn>model\")\n",
    "            parts.append(content)\n",
    "            if not last:\n",
    "                parts.append(\"<end_of_turn>\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "def _render_prompt(messages: List[dict[str, str]]) -> str:\n",
    "    \"\"\"Render a prompt using GGUF's chat template when available; fallback to a simple ChatML-ish format.\"\"\"\n",
    "    chat_template = context[\"chat_template\"]\n",
    "    if chat_template:\n",
    "        if _is_gemma3_template(chat_template):\n",
    "            return _render_gemma3_template(messages)\n",
    "        try:\n",
    "            from jinja2 import Environment, StrictUndefined\n",
    "\n",
    "            def raise_exception(msg: str):\n",
    "                raise RuntimeError(msg)\n",
    "\n",
    "            env = Environment(\n",
    "                undefined=StrictUndefined, trim_blocks=True, lstrip_blocks=True\n",
    "            )\n",
    "            j = env.from_string(chat_template)\n",
    "            return j.render(\n",
    "                messages=messages,\n",
    "                add_generation_prompt=True,\n",
    "                raise_exception=raise_exception,\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    parts: List[str] = []\n",
    "    for m in messages:\n",
    "        parts.append(f\"<|im_start|>{m['role']}\\n{m['content']}\\n<|im_end|>\")\n",
    "    parts.append(\"<|im_start|>assistant\\n\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "def _get_llama(gguf_path: Path):\n",
    "    global _LLAMA\n",
    "    if _LLAMA is not None:\n",
    "        return _LLAMA\n",
    "    try:\n",
    "        from llama_cpp import Llama\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"Missing dependency: llama-cpp-python. Install it in this notebook kernel, e.g.\\n\"\n",
    "            \"  pip install llama-cpp-python\\n\\n\"\n",
    "            \"This still uses the Ollama-downloaded GGUF file locally and does not use the ollama Python package.\"\n",
    "        ) from e\n",
    "\n",
    "    n_ctx = 4096\n",
    "    n_threads = max(1, (os.cpu_count() or 4) - 1)\n",
    "    _LLAMA = Llama(\n",
    "        model_path=str(gguf_path),\n",
    "        n_ctx=n_ctx,\n",
    "        n_threads=n_threads,\n",
    "        logits_all=False,\n",
    "        vocab_only=False,\n",
    "    )\n",
    "    return _LLAMA\n",
    "\n",
    "\n",
    "def next_message(user_message: str, messages: List[dict[str, str]]) -> str:\n",
    "    \"\"\"\n",
    "    Chat-like helper similar to `ollama.chat`, but using the local Ollama-downloaded GGUF blob.\n",
    "    Does not import or call the `ollama` Python package.\n",
    "    \"\"\"\n",
    "    msg = user_message.strip()\n",
    "    if not msg:\n",
    "        return \"\"\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": msg})\n",
    "    prompt = _render_prompt(messages)\n",
    "\n",
    "    llm = _get_llama(context[\"gguf_path\"])\n",
    "    out = llm.create_completion(\n",
    "        prompt=prompt,\n",
    "        max_tokens=256,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        stop=[\"<|im_end|>\", \"</s>\"],\n",
    "    )\n",
    "    out = cast(dict, out)\n",
    "    text = (out.get(\"choices\", [{}])[0].get(\"text\") or \"\").strip()\n",
    "    messages.append({\"role\": \"assistant\", \"content\": text})\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "722311c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 35 key-value pairs and 883 tensors from /home/max/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                gemma3.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv   1:             gemma3.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv   2:                gemma3.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv   3:            gemma3.attention.sliding_window u32              = 1024\n",
      "llama_model_loader: - kv   4:              gemma3.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv   5:                         gemma3.block_count u32              = 34\n",
      "llama_model_loader: - kv   6:                      gemma3.context_length u32              = 131072\n",
      "llama_model_loader: - kv   7:                    gemma3.embedding_length u32              = 2560\n",
      "llama_model_loader: - kv   8:                 gemma3.feed_forward_length u32              = 10240\n",
      "llama_model_loader: - kv   9:                 gemma3.mm.tokens_per_image u32              = 256\n",
      "llama_model_loader: - kv  10:         gemma3.vision.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv  11: gemma3.vision.attention.layer_norm_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  12:                  gemma3.vision.block_count u32              = 27\n",
      "llama_model_loader: - kv  13:             gemma3.vision.embedding_length u32              = 1152\n",
      "llama_model_loader: - kv  14:          gemma3.vision.feed_forward_length u32              = 4304\n",
      "llama_model_loader: - kv  15:                   gemma3.vision.image_size u32              = 896\n",
      "llama_model_loader: - kv  16:                 gemma3.vision.num_channels u32              = 3\n",
      "llama_model_loader: - kv  17:                   gemma3.vision.patch_size u32              = 14\n",
      "llama_model_loader: - kv  18:                       general.architecture str              = gemma3\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:           tokenizer.ggml.add_padding_token bool             = false\n",
      "llama_model_loader: - kv  22:           tokenizer.ggml.add_unknown_token bool             = false\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,514906]  = [\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\", ...\n",
      "llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  29:                      tokenizer.ggml.scores arr[f32,262145]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,262145]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  31:                      tokenizer.ggml.tokens arr[str,262145]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  32:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  34:                          general.file_type u32              = 15\n",
      "llama_model_loader: - type  f32:  479 tensors\n",
      "llama_model_loader: - type  f16:  165 tensors\n",
      "llama_model_loader: - type q4_K:  205 tensors\n",
      "llama_model_loader: - type q6_K:   34 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 3.09 GiB (6.18 BPW) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Tell me a joke: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load: error loading model: error loading model hyperparameters: key not found in model: gemma3.attention.layer_norm_rms_epsilon\n",
      "llama_model_load_from_file_impl: failed to load model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to load model from file: /home/max/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m prompt = \u001b[33m\"\u001b[39m\u001b[33mTell me a joke: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUser:\u001b[39m\u001b[33m\"\u001b[39m, prompt)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAssistant:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mnext_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# print()\u001b[39;00m\n\u001b[32m      9\u001b[39m \n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# print(\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m \u001b[38;5;66;03m#     ),\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 511\u001b[39m, in \u001b[36mnext_message\u001b[39m\u001b[34m(user_message, messages)\u001b[39m\n\u001b[32m    508\u001b[39m messages.append({\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: msg})\n\u001b[32m    509\u001b[39m prompt = _render_prompt(messages)\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m llm = \u001b[43m_get_llama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgguf_path\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    512\u001b[39m out = llm.create_completion(\n\u001b[32m    513\u001b[39m     prompt=prompt,\n\u001b[32m    514\u001b[39m     max_tokens=\u001b[32m256\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    517\u001b[39m     stop=[\u001b[33m\"\u001b[39m\u001b[33m<|im_end|>\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m</s>\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    518\u001b[39m )\n\u001b[32m    519\u001b[39m out = cast(\u001b[38;5;28mdict\u001b[39m, out)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 489\u001b[39m, in \u001b[36m_get_llama\u001b[39m\u001b[34m(gguf_path)\u001b[39m\n\u001b[32m    487\u001b[39m n_ctx = \u001b[32m4096\u001b[39m\n\u001b[32m    488\u001b[39m n_threads = \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, (os.cpu_count() \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m4\u001b[39m) - \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m _LLAMA = \u001b[43mLlama\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_ctx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_ctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_all\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _LLAMA\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/github.com/vegerot/localllm/.venv/lib/python3.11/site-packages/llama_cpp/llama.py:374\u001b[39m, in \u001b[36mLlama.__init__\u001b[39m\u001b[34m(self, model_path, n_gpu_layers, split_mode, main_gpu, tensor_split, vocab_only, use_mmap, use_mlock, kv_overrides, seed, n_ctx, n_batch, n_ubatch, n_threads, n_threads_batch, rope_scaling_type, pooling_type, rope_freq_base, rope_freq_scale, yarn_ext_factor, yarn_attn_factor, yarn_beta_fast, yarn_beta_slow, yarn_orig_ctx, logits_all, embedding, offload_kqv, flash_attn, op_offload, swa_full, no_perf, last_n_tokens_size, lora_base, lora_scale, lora_path, numa, chat_format, chat_handler, draft_model, tokenizer, type_k, type_v, spm_infill, verbose, **kwargs)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(model_path):\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel path does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    372\u001b[39m \u001b[38;5;28mself\u001b[39m._model = \u001b[38;5;28mself\u001b[39m._stack.enter_context(\n\u001b[32m    373\u001b[39m     contextlib.closing(\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m         \u001b[43minternals\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLlamaModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_model\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m     )\n\u001b[32m    380\u001b[39m )\n\u001b[32m    382\u001b[39m \u001b[38;5;66;03m# Override tokenizer\u001b[39;00m\n\u001b[32m    383\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer_ = tokenizer \u001b[38;5;129;01mor\u001b[39;00m LlamaTokenizer(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/github.com/vegerot/localllm/.venv/lib/python3.11/site-packages/llama_cpp/_internals.py:58\u001b[39m, in \u001b[36mLlamaModel.__init__\u001b[39m\u001b[34m(self, path_model, params, verbose)\u001b[39m\n\u001b[32m     53\u001b[39m     model = llama_cpp.llama_model_load_from_file(\n\u001b[32m     54\u001b[39m         \u001b[38;5;28mself\u001b[39m.path_model.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m), \u001b[38;5;28mself\u001b[39m.params\n\u001b[32m     55\u001b[39m     )\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to load model from file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m vocab = llama_cpp.llama_model_get_vocab(model)\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: Failed to load model from file: /home/max/.ollama/models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25"
     ]
    }
   ],
   "source": [
    "# Example usage of `next_message` (chat-like helper backed by the local GGUF)\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "\n",
    "prompt = \"Tell me a joke: \"\n",
    "print(\"User:\", prompt)\n",
    "print(\"Assistant:\", next_message(prompt, messages))\n",
    "# print()\n",
    "\n",
    "# print(\n",
    "#     \"User: Write a Python function that computes cosine similarity between two numpy vectors.\"\n",
    "# )\n",
    "# print(\n",
    "#     \"Assistant:\",\n",
    "#     next_message(\n",
    "#         \"Write a Python function that computes cosine similarity between two numpy vectors.\",\n",
    "#         messages\n",
    "#     ),\n",
    "# )\n",
    "# print()\n",
    "\n",
    "# print(\"User: Now explain in 2 bullet points how the prompt is constructed here.\")\n",
    "# print(\n",
    "#     \"Assistant:\",\n",
    "#     next_message(\n",
    "#         \"Now explain in 2 bullet points how the prompt is constructed here.\",\n",
    "#         messages\n",
    "#     ),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12635dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3) Demo\n",
      "3a) find token IDs for inputs\n",
      "3b) find closest tokens\n",
      "\n",
      "Closest matches in the entire vocabulary:\n",
      "1. 'president' (id=59692, similarity=0.6476)\n",
      "2. '▁president' (id=6207, similarity=0.5356)\n",
      "3. '近平' (id=145720, similarity=0.5196)\n",
      "4. 'President' (id=27665, similarity=0.4860)\n",
      "5. '▁President' (id=5376, similarity=0.4747)\n",
      "6. '▁presidente' (id=28769, similarity=0.4370)\n",
      "7. '▁PRESIDENT' (id=124009, similarity=0.4338)\n",
      "8. '▁presidents' (id=81965, similarity=0.3999)\n",
      "9. '▁président' (id=59687, similarity=0.3914)\n",
      "10. '▁президента' (id=163921, similarity=0.3877)\n",
      "11. '▁prezident' (id=180283, similarity=0.3824)\n",
      "12. '▁президен' (id=210929, similarity=0.3786)\n",
      "13. '▁президент' (id=115284, similarity=0.3729)\n",
      "14. 'presidente' (id=120343, similarity=0.3694)\n",
      "15. '▁Präsident' (id=126714, similarity=0.3691)\n",
      "16. '总统' (id=115884, similarity=0.3649)\n",
      "17. '▁presiden' (id=205109, similarity=0.3618)\n",
      "18. '▁প্রেসিডেন্ট' (id=39010, similarity=0.3544)\n",
      "19. '▁Presiden' (id=108539, similarity=0.3470)\n",
      "20. '▁presidenta' (id=188388, similarity=0.3464)\n",
      "21. '▁Presidents' (id=109533, similarity=0.3407)\n",
      "22. '▁Presidente' (id=69774, similarity=0.3383)\n",
      "23. '▁Président' (id=132785, similarity=0.3297)\n",
      "24. '▁राष्ट्रपति' (id=51795, similarity=0.3268)\n",
      "25. '總統' (id=180437, similarity=0.3223)\n",
      "\n",
      "Longest token in vocabulary is 31 characters: '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      "\n",
      "Top 10 longest tokens in vocabulary:\n",
      "1. 31 chars: '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      "2. 31 chars: '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁'\n",
      "3. 31 chars: '\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
      "4. 30 chars: '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      "5. 30 chars: '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁'\n",
      "6. 30 chars: '\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
      "7. 29 chars: '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      "8. 29 chars: '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁'\n",
      "9. 29 chars: '\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'\n",
      "10. 28 chars: '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "# 3) Demo: king - man + woman (single-token lookup)\n",
    "print(\"3) Demo\")\n",
    "print(\"3a) find token IDs for inputs\")\n",
    "king_id = find_token_id(\"king\", context[\"tokens\"])\n",
    "man_id = find_token_id(\"man\", context[\"tokens\"])\n",
    "woman_id = find_token_id(\"woman\", context[\"tokens\"])\n",
    "\n",
    "king_vec = get_token_vector(king_id, context[\"token_tensor\"])\n",
    "queen_vec = get_token_vector(\n",
    "    find_token_id(\"queen\", context[\"tokens\"]), context[\"token_tensor\"]\n",
    ")\n",
    "boy_vec = get_token_vector(\n",
    "    find_token_id(\"boy\", context[\"tokens\"]), context[\"token_tensor\"]\n",
    ")\n",
    "\n",
    "man_vec = get_token_vector(man_id, context[\"token_tensor\"])\n",
    "woman_vec = get_token_vector(woman_id, context[\"token_tensor\"])\n",
    "masculinity_vector = man_vec - woman_vec\n",
    "\n",
    "hebrew_vector = get_token_vector_str(\"近平\") - get_token_vector_str(\n",
    "    \"Jinping\"\n",
    ")  # + get_token_vector_str(\"apple\", context['tokens'], context['token_tensor']) #+ get_token_vector_str(\"Playstation\", context['tokens'], context['token_tensor'])\n",
    "\n",
    "target_vector = get_token_vector_str(\"president\") + hebrew_vector\n",
    "\n",
    "ignore = []\n",
    "print(\"3b) find closest tokens\")\n",
    "matches = find_closest_tokens(\n",
    "    target_vector,\n",
    "    k=25,\n",
    "    ignore_ids=ignore,\n",
    "    vocab_size=context[\"vocab_size\"],\n",
    "    token_tensor=context[\"token_tensor\"],\n",
    "    tokens=context[\"tokens\"],\n",
    ")\n",
    "\n",
    "print(\"\\nClosest matches in the entire vocabulary:\")\n",
    "for i, (tok, score, tid) in enumerate(matches, start=1):\n",
    "    print(f\"{i}. {tok!r} (id={tid}, similarity={score:.4f})\")\n",
    "\n",
    "longest_token = max(context[\"tokens\"], key=lambda t: len(t))\n",
    "print(\n",
    "    f\"\\nLongest token in vocabulary is {len(longest_token)} characters: {longest_token!r}\"\n",
    ")\n",
    "\n",
    "ten_longest_tokens = heapq.nlargest(10, context[\"tokens\"], key=lambda t: len(t))\n",
    "print(\"\\nTop 10 longest tokens in vocabulary:\")\n",
    "for i, tok in enumerate(ten_longest_tokens, start=1):\n",
    "    print(f\"{i}. {len(tok)} chars: {tok!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646902b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "localllm (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
