{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a9207bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "787fbcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between (King - Man + Woman) and Queen: 0.7330\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = ollama.embeddings(model='embeddinggemma:300m', prompt=text)\n",
    "    return np.array(response['embedding'])\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "# 1. Generate embeddings\n",
    "king = get_embedding(\"king\")\n",
    "man = get_embedding(\"man\")\n",
    "woman = get_embedding(\"woman\")\n",
    "queen = get_embedding(\"queen\")\n",
    "\n",
    "# 2. Perform the arithmetic: King - Man + Woman\n",
    "# We are essentially \"subtracting\" the concept of masculinity \n",
    "# and \"adding\" the concept of femininity to the concept of royalty.\n",
    "masculinity_vector = man - woman\n",
    "result_vector = king - masculinity_vector\n",
    "\n",
    "# 3. Verify the result\n",
    "similarity = cosine_similarity(king, queen)\n",
    "\n",
    "print(f\"Similarity between (King - Man + Woman) and Queen: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9c53c88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Ollama model: granite-embedding:30m\n",
      "Using manifest: /home/max/.ollama/models/manifests/registry.ollama.ai/library/granite-embedding/30m\n",
      "Using GGUF: /home/max/.ollama/models/blobs/sha256-27d24c87a53d110b95abecbff83f966206857a9dc0ba1efd336d08dbd0afc833\n",
      "1) Load the GGUF and access the full token embedding matrix (quantized)\n",
      "Vocab size: 50265\n",
      "Embedding dim: 384\n",
      "Embedding quant type: 1\n",
      "2) Load tokenizer tokens so we can decode token IDs back to text\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import heapq\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import gguf\n",
    "\n",
    "# Use an Ollama model reference, e.g. \"qwen3:0.6b\" or \"gemma3:27B\".\n",
    "# This cell reads the local Ollama GGUF blob directly (no HuggingFace downloads).\n",
    "\n",
    "OLLAMA_MODEL = \"granite-embedding:30m\"\n",
    "\n",
    "MANIFESTS_ROOT = Path.home() / \".ollama/models/manifests\"\n",
    "BLOBS_ROOT = Path.home() / \".ollama/models/blobs\"\n",
    "\n",
    "def _parse_ollama_model_ref(model: str) -> Tuple[str, str, str, str]:\n",
    "    \"\"\"\n",
    "    Parse Ollama model reference into (host, namespace, repo_path, tag).\n",
    "\n",
    "    Supported inputs (best-effort):\n",
    "    - repo:tag                    -> registry.ollama.ai/library/repo:tag\n",
    "    - namespace/repo:tag          -> registry.ollama.ai/namespace/repo:tag\n",
    "    - host/namespace/repo:tag     -> host/namespace/repo:tag\n",
    "\n",
    "    If :tag is omitted, tag defaults to \"latest\".\n",
    "    \"\"\"\n",
    "    model = model.strip()\n",
    "    if not model:\n",
    "        raise ValueError(\"Empty OLLAMA_MODEL\")\n",
    "\n",
    "    if \":\" in model:\n",
    "        repo_part, tag = model.rsplit(\":\", 1)\n",
    "        tag = tag or \"latest\"\n",
    "    else:\n",
    "        repo_part, tag = model, \"latest\"\n",
    "\n",
    "    parts = [p for p in repo_part.split(\"/\") if p]\n",
    "    if len(parts) >= 3:\n",
    "        host = parts[0]\n",
    "        namespace = parts[1]\n",
    "        repo_path = \"/\".join(parts[2:])\n",
    "    elif len(parts) == 2:\n",
    "        host = \"registry.ollama.ai\"\n",
    "        namespace = parts[0]\n",
    "        repo_path = parts[1]\n",
    "    else:\n",
    "        host = \"registry.ollama.ai\"\n",
    "        namespace = \"library\"\n",
    "        repo_path = parts[0]\n",
    "\n",
    "    return host, namespace, repo_path, tag\n",
    "\n",
    "def _candidate_manifest_paths(model: str) -> List[Path]:\n",
    "    host, namespace, repo_path, tag = _parse_ollama_model_ref(model)\n",
    "    repo_parts = repo_path.split(\"/\")\n",
    "\n",
    "    candidates: List[Path] = []\n",
    "    direct = MANIFESTS_ROOT / host / namespace\n",
    "    for rp in repo_parts:\n",
    "        direct = direct / rp\n",
    "    direct = direct / tag\n",
    "    candidates.append(direct)\n",
    "\n",
    "    # Fallback: match any manifest that ends with /<repo_last>/<tag>.\n",
    "    repo_last = repo_parts[-1]\n",
    "    candidates.extend(MANIFESTS_ROOT.glob(f\"**/{repo_last}/{tag}\"))\n",
    "\n",
    "    seen = set()\n",
    "    out: List[Path] = []\n",
    "    for p in candidates:\n",
    "        if p not in seen:\n",
    "            seen.add(p)\n",
    "            out.append(p)\n",
    "    return out\n",
    "\n",
    "def _resolve_ollama_manifest(model: str) -> Path:\n",
    "    candidates = _candidate_manifest_paths(model)\n",
    "    existing = [p for p in candidates if p.is_file()]\n",
    "    if not existing:\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not find an Ollama manifest for model \"\n",
    "            f\"{model!r} under {MANIFESTS_ROOT}. Tried:\\n\"\n",
    "            + \"\\n\".join(str(p) for p in candidates[:10])\n",
    "        )\n",
    "    if len(existing) == 1:\n",
    "        return existing[0]\n",
    "    existing.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return existing[0]\n",
    "\n",
    "def _resolve_ollama_gguf_blob(manifest_path: Path) -> Path:\n",
    "    obj = json.loads(manifest_path.read_text(encoding=\"utf-8\"))\n",
    "    for layer in obj.get(\"layers\", []):\n",
    "        if layer.get(\"mediaType\") == \"application/vnd.ollama.image.model\":\n",
    "            digest = layer[\"digest\"]  # e.g. sha256:...\n",
    "            if not digest.startswith(\"sha256:\"):\n",
    "                raise ValueError(f\"Unexpected digest format: {digest}\")\n",
    "            blob = BLOBS_ROOT / (\"sha256-\" + digest.split(\":\", 1)[1])\n",
    "            if not blob.exists():\n",
    "                raise FileNotFoundError(f\"Resolved blob does not exist: {blob}\")\n",
    "            return blob\n",
    "    raise ValueError(f\"No model layer found in manifest: {manifest_path}\")\n",
    "\n",
    "def _assert_gguf_file(path: Path) -> None:\n",
    "    with path.open(\"rb\") as f:\n",
    "        magic = f.read(4)\n",
    "    assert magic == b\"GGUF\", f\"Expected GGUF file, got magic={magic!r} at {path}\"\n",
    "\n",
    "manifest_path = _resolve_ollama_manifest(OLLAMA_MODEL)\n",
    "gguf_path = _resolve_ollama_gguf_blob(manifest_path)\n",
    "_assert_gguf_file(gguf_path)\n",
    "\n",
    "print(\"Using Ollama model:\", OLLAMA_MODEL)\n",
    "print(\"Using manifest:\", manifest_path)\n",
    "print(\"Using GGUF:\", gguf_path)\n",
    "\n",
    "# 1) Load the GGUF and access the full token embedding matrix (quantized)\n",
    "print(\"1) Load the GGUF and access the full token embedding matrix (quantized)\")\n",
    "reader = gguf.GGUFReader(str(gguf_path))\n",
    "token_tensor = next(t for t in reader.tensors if t.name == \"token_embd.weight\")\n",
    "vocab_size = token_tensor.data.shape[0]\n",
    "\n",
    "# Determine embedding dimensionality from a dequantized row (robust across architectures).\n",
    "embed_dim = int(gguf.dequantize(token_tensor.data[:1], token_tensor.tensor_type).shape[1])\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Embedding dim: {embed_dim}\")\n",
    "print(f\"Embedding quant type: {token_tensor.tensor_type}\")\n",
    "\n",
    "# 2) Load tokenizer tokens so we can decode token IDs back to text\n",
    "print(\"2) Load tokenizer tokens so we can decode token IDs back to text\")\n",
    "tok_field = reader.fields[\"tokenizer.ggml.tokens\"]\n",
    "tokens_raw: Union[List[str], List[bytes]] = tok_field.contents()\n",
    "\n",
    "def _tok_to_str(x: Union[str, bytes]) -> str:\n",
    "    if isinstance(x, bytes):\n",
    "        return x.decode(\"utf-8\", errors=\"replace\")\n",
    "    return str(x)\n",
    "\n",
    "tokens: List[str] = [_tok_to_str(x) for x in tokens_raw]\n",
    "# assert len(tokens) == vocab_size, \"Token list size must match embedding vocab size\"\n",
    "\n",
    "\n",
    "def find_token_id(word: str) -> int:\n",
    "    \"\"\"Best-effort lookup for a *single token* ID (not full tokenization).\"\"\"\n",
    "    candidates = [word, \"Ġ\" + word, \"▁\" + word, \" \" + word]\n",
    "    for c in candidates:\n",
    "        try:\n",
    "            return tokens.index(c)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    raise ValueError(f\"Could not find a single-token match for {word!r}. Try another word.\")\n",
    "\n",
    "def dequantize_rows(start: int, end: int) -> np.ndarray:\n",
    "    \"\"\"Dequantize embedding rows [start:end) to float32 shape [N, embed_dim].\"\"\"\n",
    "    q = token_tensor.data[start:end]\n",
    "    return gguf.dequantize(q, token_tensor.tensor_type)\n",
    "\n",
    "def get_token_vector(token_id: int) -> np.ndarray:\n",
    "    return dequantize_rows(token_id, token_id + 1)[0]\n",
    "\n",
    "def get_token_vector_str(token: str) -> np.ndarray:\n",
    "    tid = find_token_id(token)\n",
    "    return get_token_vector(tid)\n",
    "\n",
    "def normalize(v: np.ndarray) -> np.ndarray:\n",
    "    n = np.linalg.norm(v)\n",
    "    return v if n == 0 else (v / n)\n",
    "\n",
    "def find_closest_tokens(\n",
    "    vector: np.ndarray,\n",
    "    k: int = 5,\n",
    "    ignore_ids: Optional[Iterable[int]] = None,\n",
    "    chunk_size: int = 2048,\n",
    ") -> List[Tuple[str, float, int]]:\n",
    "    \"\"\"\n",
    "    Find top-k cosine-similar tokens across the *entire* vocab by dequantizing in chunks.\n",
    "    Returns (token_string, similarity, token_id).\n",
    "    \"\"\"\n",
    "    ignore = set(ignore_ids or [])\n",
    "    target = normalize(vector).astype(np.float32, copy=False)\n",
    "\n",
    "    heap: List[Tuple[float, int]] = []  # (score, token_id), min-heap\n",
    "\n",
    "    for start in range(0, vocab_size, chunk_size):\n",
    "        end = min(start + chunk_size, vocab_size)\n",
    "        emb = dequantize_rows(start, end) # float32  [N, embed_dim]\n",
    "        norms = np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "        norms[norms == 0] = 1.0\n",
    "        emb = emb / norms\n",
    "        scores = emb @ target # float32 [N]\n",
    "\n",
    "        for i, score in enumerate(scores):\n",
    "            tid = start + i\n",
    "            if tid in ignore:\n",
    "                continue\n",
    "            s = float(score)\n",
    "            if len(heap) < k:\n",
    "                heapq.heappush(heap, (s, tid))\n",
    "            elif s > heap[0][0]:\n",
    "                heapq.heapreplace(heap, (s, tid))\n",
    "\n",
    "    best = sorted(heap, reverse=True)\n",
    "    return [(tokens[tid], score, tid) for score, tid in best]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "12635dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3) Demo\n",
      "3a) find token IDs for inputs\n",
      "3b) find closest tokens\n",
      "\n",
      "Closest matches in the entire vocabulary:\n",
      "1. 'Paris' (id=32826, similarity=0.6739)\n",
      "2. 'Germany' (id=24596, similarity=0.5715)\n",
      "3. 'ĠParis' (id=2201, similarity=0.5415)\n",
      "4. 'ĠGermany' (id=1600, similarity=0.4718)\n",
      "5. 'ĠBerlin' (id=5459, similarity=0.4625)\n",
      "6. 'ĠGerman' (id=1859, similarity=0.4014)\n",
      "7. 'German' (id=27709, similarity=0.3981)\n",
      "8. 'Chicago' (id=21897, similarity=0.3699)\n",
      "9. 'ĠHamburg' (id=17938, similarity=0.3674)\n",
      "10. 'utsche' (id=35816, similarity=0.3509)\n",
      "11. 'ĠChicago' (id=1568, similarity=0.3423)\n",
      "12. 'ĠFrankfurt' (id=14184, similarity=0.3395)\n",
      "13. 'stanbul' (id=46770, similarity=0.3372)\n",
      "14. 'ĠDresden' (id=39289, similarity=0.3319)\n",
      "15. 'ĠDeutsche' (id=8043, similarity=0.3265)\n",
      "16. 'ĠIstanbul' (id=12275, similarity=0.3245)\n",
      "17. 'furt' (id=45464, similarity=0.3198)\n",
      "18. 'ĠMunich' (id=10489, similarity=0.3192)\n",
      "19. 'ĠGermans' (id=18415, similarity=0.3167)\n",
      "20. 'London' (id=23122, similarity=0.3167)\n",
      "21. 'Karl' (id=43397, similarity=0.3021)\n",
      "22. 'ĠLondon' (id=928, similarity=0.2998)\n",
      "23. 'Dallas' (id=36128, similarity=0.2941)\n",
      "24. 'Madison' (id=34570, similarity=0.2941)\n",
      "25. 'ĠSydney' (id=4290, similarity=0.2903)\n",
      "26. 'ĠBrussels' (id=6497, similarity=0.2788)\n",
      "27. 'ĠStockholm' (id=18850, similarity=0.2784)\n",
      "28. 'dam' (id=15177, similarity=0.2751)\n",
      "29. 'ĠKens' (id=15485, similarity=0.2736)\n",
      "30. 'ĠTokyo' (id=5308, similarity=0.2729)\n",
      "31. 'erman' (id=7043, similarity=0.2727)\n",
      "32. 'ĠDallas' (id=3160, similarity=0.2707)\n",
      "33. 'Moscow' (id=41384, similarity=0.2707)\n",
      "34. 'ondon' (id=24639, similarity=0.2672)\n",
      "35. 'ĠPetersburg' (id=14330, similarity=0.2662)\n",
      "36. 'ĠMadrid' (id=3622, similarity=0.2642)\n",
      "37. 'Charl' (id=33193, similarity=0.2632)\n",
      "38. 'icago' (id=42938, similarity=0.2610)\n",
      "39. 'ĠMoscow' (id=3467, similarity=0.2608)\n",
      "40. 'ĠBarcelona' (id=4612, similarity=0.2575)\n",
      "41. 'ĠCopenhagen' (id=22843, similarity=0.2572)\n",
      "42. 'ĠBrisbane' (id=10157, similarity=0.2562)\n",
      "43. 'ĠHeidi' (id=19259, similarity=0.2558)\n",
      "44. 'ĠBarbar' (id=40482, similarity=0.2541)\n",
      "45. 'ONDON' (id=4524, similarity=0.2525)\n",
      "46. 'ĠVienna' (id=15534, similarity=0.2523)\n",
      "47. 'Ġhamb' (id=31599, similarity=0.2510)\n",
      "48. 'ĠKarl' (id=8328, similarity=0.2510)\n",
      "49. 'zig' (id=22330, similarity=0.2504)\n",
      "50. 'Ġparadise' (id=26215, similarity=0.2496)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3) Demo: king - man + woman (single-token lookup)\n",
    "print(\"3) Demo\")\n",
    "print(\"3a) find token IDs for inputs\")\n",
    "king_id = find_token_id(\"king\")\n",
    "man_id = find_token_id(\"man\")\n",
    "woman_id = find_token_id(\"woman\")\n",
    "\n",
    "king_vec = get_token_vector(king_id)\n",
    "queen_vec = get_token_vector(find_token_id(\"queen\"))\n",
    "boy_vec = get_token_vector(find_token_id(\"boy\"))\n",
    "\n",
    "man_vec = get_token_vector(man_id)\n",
    "woman_vec = get_token_vector(woman_id)\n",
    "masculinity_vector = man_vec - woman_vec\n",
    "\n",
    "target_vector = queen_vec + masculinity_vector\n",
    "# target_vector = get_token_vector(find_token_id(\"The best selling book about wizards was written by JK Rowling and is called \"))\n",
    "target_vector = get_token_vector_str(\"Paris\") - get_token_vector_str(\"France\") + get_token_vector_str(\"Germany\")\n",
    "\n",
    "ignore = []\n",
    "print(\"3b) find closest tokens\")\n",
    "matches = find_closest_tokens(target_vector, k=50, ignore_ids=ignore)\n",
    "\n",
    "print(\"\\nClosest matches in the entire vocabulary:\")\n",
    "for i, (tok, score, tid) in enumerate(matches, start=1):\n",
    "    print(f\"{i}. {tok!r} (id={tid}, similarity={score:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "localllm (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
