{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a9207bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "787fbcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between (King - Man + Woman) and Queen: 0.7030\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = ollama.embeddings(model='embeddinggemma:300m', prompt=text)\n",
    "    return np.array(response['embedding'])\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "# 1. Generate embeddings\n",
    "king = get_embedding(\"king\")\n",
    "man = get_embedding(\"man\")\n",
    "woman = get_embedding(\"woman\")\n",
    "queen = get_embedding(\"queen\")\n",
    "\n",
    "# 2. Perform the arithmetic: King - Man + Woman\n",
    "# We are essentially \"subtracting\" the concept of masculinity \n",
    "# and \"adding\" the concept of femininity to the concept of royalty.\n",
    "masculinity_vector = man - woman\n",
    "result_vector = king - masculinity_vector\n",
    "\n",
    "# 3. Verify the result\n",
    "similarity = cosine_similarity(result_vector, queen)\n",
    "\n",
    "print(f\"Similarity between (King - Man + Woman) and Queen: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c53c88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GGUF: /home/max/.ollama/models/blobs/sha256-7f4030143c1c477224c5434f8272c662a8b042079a0a584f0a27a1684fe2e1fa\n",
      "Vocab size: 151936\n",
      "Embedding dim: 1024\n",
      "Embedding quant type: 12\n",
      "\n",
      "Math: king - man + woman\n",
      "\n",
      "Closest matches in the entire vocabulary:\n",
      "1. 'queen' (id=93114, similarity=0.4361)\n",
      "2. 'ĠKING' (id=73811, similarity=0.4006)\n",
      "3. 'women' (id=64662, similarity=0.3910)\n",
      "4. 'King' (id=33555, similarity=0.3866)\n",
      "5. 'Ġqueen' (id=27906, similarity=0.3709)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import heapq\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import gguf\n",
    "\n",
    "# This cell uses the *Ollama-downloaded* model directly (no HuggingFace).\n",
    "# For qwen3:0.6b, Ollama stores a GGUF blob under ~/.ollama/models/blobs/ ...\n",
    "\n",
    "OLLAMA_MANIFEST = (\n",
    "    Path.home() / \".ollama/models/manifests/registry.ollama.ai/library/qwen3/0.6b\"\n",
    ")\n",
    "\n",
    "\n",
    "def _resolve_ollama_gguf_blob(manifest_path: Path) -> Path:\n",
    "    obj = json.loads(manifest_path.read_text(encoding=\"utf-8\"))\n",
    "    for layer in obj.get(\"layers\", []):\n",
    "        if layer.get(\"mediaType\") == \"application/vnd.ollama.image.model\":\n",
    "            digest = layer[\"digest\"]  # e.g. sha256:...\n",
    "            if not digest.startswith(\"sha256:\"):\n",
    "                raise ValueError(f\"Unexpected digest format: {digest}\")\n",
    "            blob = (\n",
    "                Path.home()\n",
    "                / \".ollama/models/blobs\"\n",
    "                / (\"sha256-\" + digest.split(\":\", 1)[1])\n",
    "            )\n",
    "            if not blob.exists():\n",
    "                raise FileNotFoundError(f\"Resolved blob does not exist: {blob}\")\n",
    "            return blob\n",
    "    raise ValueError(f\"No model layer found in manifest: {manifest_path}\")\n",
    "\n",
    "\n",
    "GGUF_PATH = _resolve_ollama_gguf_blob(OLLAMA_MANIFEST)\n",
    "print(\"Using GGUF:\", GGUF_PATH)\n",
    "\n",
    "# 1) Load the GGUF and grab the full token embedding matrix (quantized)\n",
    "reader = gguf.GGUFReader(str(GGUF_PATH))\n",
    "token_tensor = next(t for t in reader.tensors if t.name == \"token_embd.weight\")\n",
    "vocab_size = token_tensor.data.shape[0]\n",
    "\n",
    "# Determine the dequantized embedding dimensionality robustly.\n",
    "embed_dim = int(\n",
    "    gguf.dequantize(token_tensor.data[:1], token_tensor.tensor_type).shape[1]\n",
    ")\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Embedding dim: {embed_dim}\")\n",
    "print(f\"Embedding quant type: {token_tensor.tensor_type}\")\n",
    "\n",
    "# 2) Load tokenizer token strings so we can decode token IDs back to text\n",
    "tok_field = reader.fields[\"tokenizer.ggml.tokens\"]\n",
    "tokens_raw: Union[List[str], List[bytes]] = tok_field.contents()\n",
    "\n",
    "\n",
    "def _tok_to_str(x: Union[str, bytes]) -> str:\n",
    "    if isinstance(x, bytes):\n",
    "        return x.decode(\"utf-8\", errors=\"replace\")\n",
    "    return str(x)\n",
    "\n",
    "\n",
    "tokens: List[str] = [_tok_to_str(x) for x in tokens_raw]\n",
    "assert len(tokens) == vocab_size, \"Token list size must match embedding vocab size\"\n",
    "\n",
    "\n",
    "def find_token_id(word: str) -> int:\n",
    "    \"\"\"Best-effort lookup for a *single token* ID (not full tokenization).\"\"\"\n",
    "    candidates = [word, \"Ġ\" + word, \"▁\" + word, \" \" + word]\n",
    "    for c in candidates:\n",
    "        try:\n",
    "            return tokens.index(c)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    raise ValueError(\n",
    "        f\"Could not find a single-token match for {word!r}. Try another word.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def dequantize_rows(start: int, end: int) -> np.ndarray:\n",
    "    \"\"\"Dequantize embedding rows [start:end) to float32 shape [N, embed_dim].\"\"\"\n",
    "    q = token_tensor.data[start:end]  # uint8, quant-packed\n",
    "    return gguf.dequantize(q, token_tensor.tensor_type)\n",
    "\n",
    "\n",
    "def get_token_vector(token_id: int) -> np.ndarray:\n",
    "    return dequantize_rows(token_id, token_id + 1)[0]\n",
    "\n",
    "\n",
    "def normalize(v: np.ndarray) -> np.ndarray:\n",
    "    n = np.linalg.norm(v)\n",
    "    return v if n == 0 else (v / n)\n",
    "\n",
    "\n",
    "def find_closest_tokens(\n",
    "    vector: np.ndarray,\n",
    "    k: int = 5,\n",
    "    ignore_ids: Optional[Iterable[int]] = None,\n",
    "    chunk_size: int = 2048,\n",
    ") -> List[Tuple[str, float, int]]:\n",
    "    \"\"\"\n",
    "    Approximate 'unembedding': find top-k cosine-similar tokens across the *entire* vocab.\n",
    "    Works by dequantizing the GGUF embedding matrix in chunks to avoid huge RAM spikes.\n",
    "    Returns (token_string, similarity, token_id).\n",
    "    \"\"\"\n",
    "    ignore = set(ignore_ids or [])\n",
    "    target = normalize(vector).astype(np.float32, copy=False)\n",
    "\n",
    "    heap: List[Tuple[float, int]] = []  # (score, token_id), min-heap\n",
    "\n",
    "    for start in range(0, vocab_size, chunk_size):\n",
    "        end = min(start + chunk_size, vocab_size)\n",
    "        emb = dequantize_rows(start, end)  # float32 [N, D]\n",
    "        norms = np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "        norms[norms == 0] = 1.0\n",
    "        emb = emb / norms\n",
    "        scores = emb @ target  # [N]\n",
    "\n",
    "        for i, score in enumerate(scores):\n",
    "            tid = start + i\n",
    "            if tid in ignore:\n",
    "                continue\n",
    "            s = float(score)\n",
    "            if len(heap) < k:\n",
    "                heapq.heappush(heap, (s, tid))\n",
    "            elif s > heap[0][0]:\n",
    "                heapq.heapreplace(heap, (s, tid))\n",
    "\n",
    "    best = sorted(heap, reverse=True)\n",
    "    return [(tokens[tid], score, tid) for score, tid in best]\n",
    "\n",
    "\n",
    "# 3) Demo: King - Man + Woman (single-token lookup)\n",
    "king_id = find_token_id(\"king\")\n",
    "man_id = find_token_id(\"man\")\n",
    "woman_id = find_token_id(\"woman\")\n",
    "\n",
    "king_vec = get_token_vector(king_id)\n",
    "man_vec = get_token_vector(man_id)\n",
    "woman_vec = get_token_vector(woman_id)\n",
    "\n",
    "target_vector = king_vec - man_vec + woman_vec\n",
    "\n",
    "ignore = [king_id, man_id, woman_id]\n",
    "print(\"\\nMath: king - man + woman\")\n",
    "matches = find_closest_tokens(target_vector, k=5, ignore_ids=ignore, chunk_size=2048)\n",
    "\n",
    "print(\"\\nClosest matches in the entire vocabulary:\")\n",
    "for i, (tok, score, tid) in enumerate(matches, start=1):\n",
    "    print(f\"{i}. {tok!r} (id={tid}, similarity={score:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "localllm (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
